{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8mB57Yr0cko",
        "outputId": "3bda1f93-8e31-4d86-8308-c235570ac08f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hello"
      ],
      "metadata": {
        "id": "Yf1j8lLd20qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F"
      ],
      "metadata": {
        "id": "KUEYs8zZ4-D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ================================\n",
        "# # ðŸ”¥ PYSPARK MEGA PRACTICE NOTEBOOK (Beginner â†’ Intermediate)\n",
        "# # One-cell, comment/uncomment based learning plan (7 Days)\n",
        "# # Works best on Google Colab\n",
        "# # ================================\n",
        "\n",
        "# # ================================\n",
        "# # 0) INSTALL & SETUP\n",
        "# # ================================\n",
        "# # If running on Colab, install pyspark (skip on Databricks or local where Spark is preinstalled)\n",
        "# # !pip -q install pyspark==3.5.1\n",
        "\n",
        "import os, io, json, math, random, pandas as pd, numpy as np\n",
        "from datetime import datetime, date, timedelta\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "\n",
        "# ==== Start Spark Session ====\n",
        "spark = SparkSession.builder.appName(\"GitHub_PySpark\").getOrCreate()\n",
        "\n",
        "# ==== GitHub RAW URLs ====\n",
        "base = \"https://raw.githubusercontent.com/theabhinaykumar/csv/main/\"\n",
        "\n",
        "user_reviews   = base + \"user_reviews.csv\"\n",
        "apps           = base + \"apps.csv\"\n",
        "db1_sales      = base + \"DB1%20sales.csv\"\n",
        "hospital       = base + \"hospital.csv\"\n",
        "ab_nyc_2019    = base + \"AB_NYC_2019.csv\"\n",
        "creditcard     = base + \"creditcard.csv\"\n",
        "creditcard_1   = base + \"creditcard%201.csv\"\n",
        "\n",
        "# ==== Load CSVs into Spark DataFrames ====\n",
        "df_reviews   = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(user_reviews)\n",
        "df_apps      = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(apps)\n",
        "df_db1sales  = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(db1_sales)\n",
        "df_hospital  = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(hospital)\n",
        "df_abnyc     = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(ab_nyc_2019)\n",
        "df_credit    = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(creditcard)\n",
        "df_credit_1  = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(creditcard_1)\n",
        "\n",
        "# ==== Check data ====\n",
        "df_reviews.show(5)\n",
        "df_apps.show(5)\n",
        "df_hospital.printSchema()\n",
        "df_abnyc.describe().show()\n",
        "df_credit.count()\n",
        "\n",
        "\n",
        "# spark = (SparkSession.builder\n",
        "#          .appName(\"PySpark_Mega_Practice\")\n",
        "#          # .config(\"spark.sql.shuffle.partitions\", \"200\")  # Tune if needed\n",
        "#          .getOrCreate())\n",
        "\n",
        "# print(\"âœ… Spark version:\", spark.version)\n",
        "# sc = spark.sparkContext\n",
        "# sc.setLogLevel(\"WARN\")\n",
        "\n",
        "# # OPTIONAL: checkpoint (useful for complex pipelines)\n",
        "# spark.sparkContext.setCheckpointDir(\"/tmp/spark_checkpoints\")\n",
        "\n",
        "# # ================================\n",
        "# # 1) DATA LOADING OPTIONS\n",
        "# # ================================\n",
        "# # Choose one: \"github\" (raw URL), \"local_upload\" (manual upload), \"drive\" (Google Drive path)\n",
        "# SOURCE = \"github\"  # â† change to \"local_upload\" or \"drive\" when needed\n",
        "\n",
        "# # GitHub RAW CSV (public dataset). You can replace with your own RAW CSV link.\n",
        "# GITHUB_RAW_CSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv\"  # small dataset\n",
        "\n",
        "# # If using Google Drive, mount and set your CSV path (example):\n",
        "# DRIVE_CSV_PATH = \"/content/drive/MyDrive/data/myfile.csv\"\n",
        "\n",
        "# # ---- Helpers ----\n",
        "# def load_from_github(url: str):\n",
        "#     \"\"\"\n",
        "#     Best-practice for GitHub CSV on Colab:\n",
        "#     - pandas.read_csv over HTTPS\n",
        "#     - convert pandas â†’ Spark\n",
        "#     \"\"\"\n",
        "#     pdf = pd.read_csv(url)\n",
        "#     return spark.createDataFrame(pdf)\n",
        "\n",
        "# def load_from_local_upload():\n",
        "#     \"\"\"\n",
        "#     Upload from your laptop to Colab.\n",
        "#     It will prompt a file chooser UI.\n",
        "#     \"\"\"\n",
        "#     from google.colab import files  # works on Colab\n",
        "#     uploaded = files.upload()\n",
        "#     if not uploaded:\n",
        "#         raise RuntimeError(\"No file uploaded.\")\n",
        "#     fname = list(uploaded.keys())[0]\n",
        "#     print(\"Uploaded:\", fname)\n",
        "#     return (spark.read\n",
        "#             .option(\"header\", True)\n",
        "#             .option(\"inferSchema\", True)\n",
        "#             .option(\"multiLine\", True)\n",
        "#             .option(\"escape\", \"\\\"\")\n",
        "#             .csv(fname))\n",
        "\n",
        "# def load_from_drive(path: str):\n",
        "#     \"\"\"\n",
        "#     Read a CSV already present on filesystem (e.g., Drive).\n",
        "#     \"\"\"\n",
        "#     if not os.path.isdir(\"/content/drive\"):\n",
        "#         from google.colab import drive\n",
        "#         drive.mount('/content/drive', force_remount=True)\n",
        "#     return (spark.read\n",
        "#             .option(\"header\", True)\n",
        "#             .option(\"inferSchema\", True)\n",
        "#             .option(\"multiLine\", True)\n",
        "#             .option(\"escape\", \"\\\"\")\n",
        "#             .csv(path))\n",
        "\n",
        "# # ---- Load main DataFrame: df ----\n",
        "# if SOURCE == \"github\":\n",
        "#     df = load_from_github(GITHUB_RAW_CSV_URL)\n",
        "# elif SOURCE == \"local_upload\":\n",
        "#     df = load_from_local_upload()\n",
        "# elif SOURCE == \"drive\":\n",
        "#     df = load_from_drive(DRIVE_CSV_PATH)\n",
        "# else:\n",
        "#     raise ValueError(\"SOURCE must be 'github', 'local_upload', or 'drive'\")\n",
        "\n",
        "# print(\"\\n=== DATA PREVIEW ===\")\n",
        "# print(\"Rows:\", df.count())\n",
        "# print(\"Columns:\", df.columns)\n",
        "# df.printSchema()\n",
        "# df.show(5, truncate=False)\n",
        "\n",
        "# # Cache for faster iterative analysis\n",
        "# df.cache()\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # DAY 1 â€” BASICS: view, schema, select, filter, simple stats\n",
        "# # =========================================================\n",
        "\n",
        "# # UNCOMMENT this block to practice Day-1\n",
        "# # \"\"\"\n",
        "# print(\"\\n================= DAY 1 =================\")\n",
        "# # Show top rows\n",
        "# df.show(10)\n",
        "\n",
        "# # Schema (column names + data types)\n",
        "# df.printSchema()\n",
        "\n",
        "# # Column list, count\n",
        "# print(\"Total Rows:\", df.count())\n",
        "# print(\"Total Cols :\", len(df.columns))\n",
        "# print(\"Columns    :\", df.columns)\n",
        "\n",
        "# # Select specific cols (change as per your CSV)\n",
        "# some_cols = df.columns[:3]  # first 3 columns for demo\n",
        "# df.select(*some_cols).show(5)\n",
        "\n",
        "# # Rename columns safely (example)\n",
        "# renamed_df = df\n",
        "# # Example: if your dataset has \"size (cm)\" type names, you can rename like:\n",
        "# # for c in df.columns:\n",
        "# #     renamed_df = renamed_df.withColumnRenamed(c, c.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\"))\n",
        "\n",
        "# # Basic describe (numeric cols only)\n",
        "# print(\"\\n--- Describe numeric cols ---\")\n",
        "# df.describe().show()\n",
        "\n",
        "# # Simple filtering (example numeric col: 'total_bill' if using tips.csv)\n",
        "# num_col = None\n",
        "# for c, t in df.dtypes:\n",
        "#     if t in (\"int\", \"bigint\", \"double\", \"float\", \"long\", \"decimal\"):\n",
        "#         num_col = c\n",
        "#         break\n",
        "\n",
        "# if num_col:\n",
        "#     print(f\"\\n--- Filter {num_col} > average ---\")\n",
        "#     avg_val = df.select(F.avg(F.col(num_col))).first()[0]\n",
        "#     df.filter(F.col(num_col) > avg_val).show(5)\n",
        "\n",
        "# # Distinct counts per column\n",
        "# print(\"\\n--- Distinct counts per column ---\")\n",
        "# distincts = df.select([F.countDistinct(F.col(c)).alias(c) for c in df.columns])\n",
        "# distincts.show()\n",
        "\n",
        "# # Sort (orderBy)\n",
        "# if num_col:\n",
        "#     print(f\"\\n--- Top 5 rows by {num_col} desc ---\")\n",
        "#     df.orderBy(F.col(num_col).desc()).show(5)\n",
        "# # \"\"\"\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # DAY 2 â€” SELECT/ALIAS, WHERE/FILTER, BOOLEAN LOGIC, LIKE/IN/IS NULL\n",
        "# # =========================================================\n",
        "\n",
        "# # UNCOMMENT to practice Day-2\n",
        "# # \"\"\"\n",
        "# print(\"\\n================= DAY 2 =================\")\n",
        "# # Aliasing and expressions\n",
        "# demo = df.select(\n",
        "#     *[F.col(c) for c in df.columns],\n",
        "# )\n",
        "\n",
        "# # Example: Add a constant column (lit), and create boolean conditions\n",
        "# demo = demo.withColumn(\"CONST_ONE\", F.lit(1))\n",
        "\n",
        "# # String search (if any string column exists)\n",
        "# str_col = None\n",
        "# for c, t in df.dtypes:\n",
        "#     if t == \"string\":\n",
        "#         str_col = c\n",
        "#         break\n",
        "\n",
        "# if str_col:\n",
        "#     print(f\"Using string column: {str_col}\")\n",
        "#     demo.filter(F.col(str_col).isNotNull()).select(str_col).show(5)\n",
        "#     demo.filter(F.col(str_col).like(\"%a%\")).select(str_col).show(5)  # contains letter 'a'\n",
        "#     demo.filter(F.col(str_col).isin(demo.select(str_col).limit(3).rdd.flatMap(lambda x: x).collect())).select(str_col).show(5)\n",
        "\n",
        "# # Null handling preview\n",
        "# print(\"\\n--- Null handling ---\")\n",
        "# print(\"Rows with ANY null in any column:\", df.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).agg(\n",
        "#     sum([F.col(c) for c in df.columns])).collect()[0][0])\n",
        "\n",
        "# null_counts = df.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
        "# null_counts.show()\n",
        "\n",
        "# # Replace nulls generically\n",
        "# filled = df.fillna(0)  # numeric nulls â†’ 0; strings stay unchanged\n",
        "# filled.show(3)\n",
        "# # \"\"\"\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # DAY 3 â€” GROUPBY/AGG, MULTI-AGG, PIVOT, HAVING\n",
        "# # =========================================================\n",
        "\n",
        "# # UNCOMMENT to practice Day-3\n",
        "# # \"\"\"\n",
        "# print(\"\\n================= DAY 3 =================\")\n",
        "# # Pick a categorical col and numeric col for aggregations\n",
        "# cat_col = None\n",
        "# for c, t in df.dtypes:\n",
        "#     if t not in (\"int\",\"bigint\",\"double\",\"float\",\"long\",\"decimal\"):\n",
        "#         cat_col = c\n",
        "#         break\n",
        "\n",
        "# num_cols = [c for c, t in df.dtypes if t in (\"int\",\"bigint\",\"double\",\"float\",\"long\",\"decimal\")]\n",
        "\n",
        "# if cat_col and num_cols:\n",
        "#     agg_df = df.groupBy(cat_col).agg(\n",
        "#         F.count(\"*\").alias(\"rows\"),\n",
        "#         *[F.round(F.avg(F.col(n)), 3).alias(f\"avg_{n}\") for n in num_cols[:2]]\n",
        "#     ).orderBy(F.col(\"rows\").desc())\n",
        "#     agg_df.show()\n",
        "\n",
        "#     # Pivot (only if a second categorical col exists)\n",
        "#     # Create a small demo: average of first numeric col by (cat_col, another_cat)\n",
        "#     other_cat = None\n",
        "#     for c, t in df.dtypes:\n",
        "#         if t not in (\"int\",\"bigint\",\"double\",\"float\",\"long\",\"decimal\") and c != cat_col:\n",
        "#             other_cat = c\n",
        "#             break\n",
        "\n",
        "#     if other_cat and len(num_cols) > 0:\n",
        "#         print(f\"\\n--- Pivot: avg({num_cols[0]}) by {cat_col} pivot {other_cat} ---\")\n",
        "#         pivot_df = (df.groupBy(cat_col)\n",
        "#                       .pivot(other_cat)\n",
        "#                       .agg(F.round(F.avg(F.col(num_cols[0])), 2)))\n",
        "#         pivot_df.show()\n",
        "\n",
        "# # Having-like filter: use filter after agg\n",
        "# if cat_col:\n",
        "#     print(\"\\n--- Having-like: keep groups with rows >= 10 ---\")\n",
        "#     gb = df.groupBy(cat_col).agg(F.count(\"*\").alias(\"rows\"))\n",
        "#     gb.filter(F.col(\"rows\") >= 10).show()\n",
        "# # \"\"\"\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # DAY 4 â€” STRING/DATE/NUMERIC FUNCTIONS, UDFs, JSON, ARRAY/EXPLODE\n",
        "# # =========================================================\n",
        "\n",
        "# # UNCOMMENT to practice Day-4\n",
        "# # \"\"\"\n",
        "# print(\"\\n================= DAY 4 =================\")\n",
        "\n",
        "# # STRING FUNCS\n",
        "# if str_col:\n",
        "#     print(\"\\n--- STRING FUNCS ---\")\n",
        "#     str_df = df.select(\n",
        "#         F.col(str_col).alias(\"raw\"),\n",
        "#         F.upper(F.col(str_col)).alias(\"upper\"),\n",
        "#         F.lower(F.col(str_col)).alias(\"lower\"),\n",
        "#         F.length(F.col(str_col)).alias(\"len\"),\n",
        "#         F.substring(F.col(str_col), 1, 3).alias(\"first3\")\n",
        "#     )\n",
        "#     str_df.show(5, truncate=False)\n",
        "\n",
        "# # DATE/TIME FUNCS â€” make a tiny demo column\n",
        "# print(\"\\n--- DATE/TIME FUNCS ---\")\n",
        "# demo_dates = spark.createDataFrame([\n",
        "#     (\"A\", \"2025-08-31 10:15:00\"),\n",
        "#     (\"B\", \"2025-09-01 22:45:33\"),\n",
        "#     (\"C\", \"2025-09-03 05:00:00\")\n",
        "# ], schema=\"id string, ts string\")\n",
        "\n",
        "# dt = (demo_dates\n",
        "#       .withColumn(\"ts_ts\", F.to_timestamp(\"ts\"))\n",
        "#       .withColumn(\"date\", F.to_date(\"ts_ts\"))\n",
        "#       .withColumn(\"year\", F.year(\"ts_ts\"))\n",
        "#       .withColumn(\"month\", F.month(\"ts_ts\"))\n",
        "#       .withColumn(\"day\", F.dayofmonth(\"ts_ts\"))\n",
        "#       .withColumn(\"dow\", F.date_format(\"ts_ts\", \"E\"))\n",
        "#       .withColumn(\"plus_7d\", F.date_add(F.col(\"date\"), 7))\n",
        "#       .withColumn(\"diff_days\", F.datediff(F.col(\"plus_7d\"), F.col(\"date\"))))\n",
        "# dt.show(truncate=False)\n",
        "\n",
        "# # NUMERIC FUNCS\n",
        "# if num_col:\n",
        "#     print(\"\\n--- NUMERIC FUNCS ---\")\n",
        "#     num_demo = df.select(\n",
        "#         F.col(num_col).alias(\"x\"),\n",
        "#         F.round(F.col(num_col), 2).alias(\"round2\"),\n",
        "#         F.sqrt(F.abs(F.col(num_col))).alias(\"sqrt_abs\"),\n",
        "#         F.when(F.col(num_col) > 10, \"big\").otherwise(\"small\").alias(\"bucket\")\n",
        "#     )\n",
        "#     num_demo.show(5)\n",
        "\n",
        "# # UDF (User Defined Function) â€“ only when truly required\n",
        "# print(\"\\n--- UDF demo (square numeric) ---\")\n",
        "# @F.udf(\"double\")\n",
        "# def square_udf(x):\n",
        "#     return float(x)**2 if x is not None else None\n",
        "\n",
        "# if num_col:\n",
        "#     df.withColumn(\"squared\", square_udf(F.col(num_col))).select(num_col, \"squared\").show(5)\n",
        "\n",
        "# # ARRAY / EXPLODE / JSON\n",
        "# print(\"\\n--- ARRAY/EXPLODE/JSON ---\")\n",
        "# arr_df = spark.createDataFrame([\n",
        "#     (1, [\"red\",\"blue\",\"green\"], '{\"k\":1,\"msg\":\"hi\"}'),\n",
        "#     (2, [\"x\",\"y\"], '{\"k\":2,\"msg\":\"yo\"}'),\n",
        "# ], schema=\"id int, colors array<string>, meta string\")\n",
        "\n",
        "# arr_df.select(\"id\", \"colors\", F.explode(\"colors\").alias(\"color\")).show()\n",
        "\n",
        "# json_df = arr_df.select(\n",
        "#     \"id\",\n",
        "#     F.from_json(\"meta\", T.StructType([\n",
        "#         T.StructField(\"k\", T.IntegerType(), True),\n",
        "#         T.StructField(\"msg\", T.StringType(), True)\n",
        "#     ])).alias(\"meta_parsed\")\n",
        "# )\n",
        "# json_df.select(\"id\", \"meta_parsed.*\").show()\n",
        "# # \"\"\"\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # DAY 5 â€” JOINS (inner, left, right, full), SEMI/ANTI, UNION, DEDUP\n",
        "# # =========================================================\n",
        "\n",
        "# # UNCOMMENT to practice Day-5\n",
        "# # \"\"\"\n",
        "# print(\"\\n================= DAY 5 =================\")\n",
        "\n",
        "# # Synthetic small tables for joins\n",
        "# customers = spark.createDataFrame([\n",
        "#     (1, \"Amit\", \"Delhi\"),\n",
        "#     (2, \"Riya\", \"Noida\"),\n",
        "#     (3, \"Karan\", \"Gurgaon\"),\n",
        "#     (4, \"Neha\", \"Delhi\"),\n",
        "# ], schema=\"cust_id int, name string, city string\")\n",
        "\n",
        "# orders = spark.createDataFrame([\n",
        "#     (101, 1, \"2025-08-30\", 2999.0),\n",
        "#     (102, 1, \"2025-09-01\", 1599.0),\n",
        "#     (103, 2, \"2025-08-29\",  899.0),\n",
        "#     (104, 5, \"2025-08-28\", 1200.0),  # cust_id=5 not in customers to demo outer join\n",
        "# ], schema=\"order_id int, cust_id int, order_date string, amount double\")\n",
        "\n",
        "# print(\"\\nCustomers:\"); customers.show()\n",
        "# print(\"Orders:\"); orders.show()\n",
        "\n",
        "# inner_j = customers.join(orders, \"cust_id\", \"inner\")\n",
        "# left_j  = customers.join(orders, \"cust_id\", \"left\")\n",
        "# right_j = customers.join(orders, \"cust_id\", \"right\")\n",
        "# full_j  = customers.join(orders, \"cust_id\", \"outer\")\n",
        "\n",
        "# print(\"\\n--- INNER JOIN ---\"); inner_j.show()\n",
        "# print(\"\\n--- LEFT JOIN ---\"); left_j.show()\n",
        "# print(\"\\n--- RIGHT JOIN ---\"); right_j.show()\n",
        "# print(\"\\n--- FULL OUTER JOIN ---\"); full_j.show()\n",
        "\n",
        "# # SEMI (keep left rows that have a match) & ANTI (keep left rows that DON'T have a match)\n",
        "# semi_j = customers.join(orders, \"cust_id\", \"left_semi\")\n",
        "# anti_j = customers.join(orders, \"cust_id\", \"left_anti\")\n",
        "# print(\"\\n--- LEFT SEMI (customers with at least 1 order) ---\"); semi_j.show()\n",
        "# print(\"\\n--- LEFT ANTI (customers with NO orders) ---\"); anti_j.show()\n",
        "\n",
        "# # UNION (stack vertically, same schema)\n",
        "# customers2 = spark.createDataFrame([\n",
        "#     (5, \"Meera\", \"Ghaziabad\"),\n",
        "#     (6, \"Vikram\", \"Faridabad\"),\n",
        "# ], schema=customers.schema)\n",
        "# all_customers = customers.unionByName(customers2)\n",
        "# print(\"\\n--- UNION customers + customers2 ---\"); all_customers.show()\n",
        "\n",
        "# # DEDUP (dropDuplicates)\n",
        "# dedup = all_customers.unionByName(customers).dropDuplicates([\"cust_id\"])\n",
        "# print(\"\\n--- DEDUP by cust_id ---\"); dedup.orderBy(\"cust_id\").show()\n",
        "# # \"\"\"\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # DAY 6 â€” WINDOW FUNCTIONS: rank, dense_rank, row_number, lag/lead, rolling\n",
        "# # =========================================================\n",
        "\n",
        "# # UNCOMMENT to practice Day-6\n",
        "# # \"\"\"\n",
        "# print(\"\\n================= DAY 6 =================\")\n",
        "# # Using 'orders' from Day-5 (re-create if you skipped Day-5)\n",
        "# orders = spark.createDataFrame([\n",
        "#     (101, 1, \"2025-08-30\", 2999.0),\n",
        "#     (102, 1, \"2025-09-01\", 1599.0),\n",
        "#     (103, 2, \"2025-08-29\",  899.0),\n",
        "#     (104, 5, \"2025-08-28\", 1200.0),\n",
        "#     (105, 2, \"2025-09-02\", 1799.0),\n",
        "#     (106, 1, \"2025-09-03\",  499.0),\n",
        "# ], schema=\"order_id int, cust_id int, order_date string, amount double\")\n",
        "\n",
        "# w_cust_date = Window.partitionBy(\"cust_id\").orderBy(F.to_date(\"order_date\"))\n",
        "\n",
        "# win_df = (orders\n",
        "#           .withColumn(\"rn\", F.row_number().over(w_cust_date))\n",
        "#           .withColumn(\"rnk\", F.rank().over(w_cust_date))\n",
        "#           .withColumn(\"dense_rnk\", F.dense_rank().over(w_cust_date))\n",
        "#           .withColumn(\"prev_amount\", F.lag(\"amount\", 1).over(w_cust_date))\n",
        "#           .withColumn(\"next_amount\", F.lead(\"amount\", 1).over(w_cust_date))\n",
        "#           .withColumn(\"diff_from_prev\", F.col(\"amount\") - F.col(\"prev_amount\"))\n",
        "#           )\n",
        "\n",
        "# print(\"\\n--- Window basics ---\"); win_df.orderBy(\"cust_id\", \"order_date\").show()\n",
        "\n",
        "# # Rolling sum (requires a range/between rows frame)\n",
        "# w_rows = Window.partitionBy(\"cust_id\").orderBy(F.to_date(\"order_date\")).rowsBetween(Window.unboundedPreceding, 0)\n",
        "# rolling = orders.withColumn(\"running_total\", F.sum(\"amount\").over(w_rows))\n",
        "# print(\"\\n--- Running total per customer ---\"); rolling.orderBy(\"cust_id\", \"order_date\").show()\n",
        "# # \"\"\"\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # DAY 7 â€” PERFORMANCE, WRITE, FILE FORMATS, REPARTITION, EXPLAIN\n",
        "# # =========================================================\n",
        "\n",
        "# # UNCOMMENT to practice Day-7\n",
        "# # \"\"\"\n",
        "# print(\"\\n================= DAY 7 =================\")\n",
        "\n",
        "# # Repartition / coalesce (control parallelism & output files)\n",
        "# print(\"\\n--- Repartition/Coalesce demo ---\")\n",
        "# r_df = df.repartition(4)      # increase partitions\n",
        "# print(\"Partitions after repartition(4):\", r_df.rdd.getNumPartitions())\n",
        "# c_df = r_df.coalesce(1)       # reduce partitions (try to write single output file)\n",
        "# print(\"Partitions after coalesce(1):\", c_df.rdd.getNumPartitions())\n",
        "\n",
        "# # Write outputs (small sample to avoid huge storage)\n",
        "# out_base = \"/content/pyspark_outputs\"\n",
        "# os.makedirs(out_base, exist_ok=True)\n",
        "\n",
        "# (df.limit(500)\n",
        "#    .write.mode(\"overwrite\").option(\"header\", True).csv(f\"{out_base}/csv_output\"))\n",
        "# print(\"âœ… Wrote CSV to:\", f\"{out_base}/csv_output\")\n",
        "\n",
        "# (df.limit(500)\n",
        "#    .write.mode(\"overwrite\").parquet(f\"{out_base}/parquet_output\"))\n",
        "# print(\"âœ… Wrote Parquet to:\", f\"{out_base}/parquet_output\")\n",
        "\n",
        "# # Explain plans (how Spark will execute)\n",
        "# print(\"\\n--- EXPLAIN plan (physical) ---\")\n",
        "# df.explain(mode=\"formatted\")\n",
        "\n",
        "# # Broadcast join hint (small table broadcast to speed up)\n",
        "# small_dim = spark.createDataFrame([(1,\"VIP\"), (2,\"REG\"), (3,\"NEW\")], \"cust_id int, segment string\")\n",
        "# orders = spark.createDataFrame([\n",
        "#     (101, 1, 2999.0),\n",
        "#     (102, 1, 1599.0),\n",
        "#     (103, 2,  899.0),\n",
        "#     (106, 1,  499.0),\n",
        "# ], \"order_id int, cust_id int, amount double\")\n",
        "\n",
        "# joined = orders.join(F.broadcast(small_dim), \"cust_id\", \"left\")\n",
        "# print(\"\\n--- Broadcast Join ---\"); joined.show()\n",
        "\n",
        "# # Basic SQL usage (optional)\n",
        "# print(\"\\n--- Spark SQL demo ---\")\n",
        "# df.createOrReplaceTempView(\"t\")\n",
        "# spark.sql(\"SELECT * FROM t LIMIT 5\").show()\n",
        "# # \"\"\"\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # MINI PROJECT â€” END-TO-END (Load â†’ Clean â†’ Transform â†’ Analyze â†’ Export)\n",
        "# # =========================================================\n",
        "\n",
        "# # UNCOMMENT to practice Mini Project\n",
        "# # \"\"\"\n",
        "# print(\"\\n================= MINI PROJECT =================\")\n",
        "\n",
        "# # 1) Load: already done into df\n",
        "\n",
        "# # 2) Clean: standardize column names (snake_case), trim strings\n",
        "# clean = df\n",
        "# for c in df.columns:\n",
        "#     new_c = (c.strip()\n",
        "#                .lower()\n",
        "#                .replace(\" \", \"_\")\n",
        "#                .replace(\"(\", \"\")\n",
        "#                .replace(\")\", \"\")\n",
        "#                .replace(\"-\", \"_\"))\n",
        "#     clean = clean.withColumnRenamed(c, new_c)\n",
        "\n",
        "# # Trim string columns & handle nulls\n",
        "# for c, t in clean.dtypes:\n",
        "#     if t == \"string\":\n",
        "#         clean = clean.withColumn(c, F.trim(F.col(c)))\n",
        "\n",
        "# # 3) Transform: demo features (example assumes there is at least one numeric col)\n",
        "# # Add: zscore of first numeric column + bucketize\n",
        "# num_cols = [c for c, t in clean.dtypes if t in (\"int\",\"bigint\",\"double\",\"float\",\"long\",\"decimal\")]\n",
        "# if num_cols:\n",
        "#     x = num_cols[0]\n",
        "#     stats = clean.select(F.avg(x).alias(\"mu\"), F.stddev(x).alias(\"sd\")).first()\n",
        "#     mu, sd = stats[\"mu\"], stats[\"sd\"] or 1.0\n",
        "#     clean = (clean\n",
        "#              .withColumn(f\"{x}_z\", (F.col(x) - F.lit(mu)) / F.lit(sd))\n",
        "#              .withColumn(f\"{x}_bucket\", F.when(F.col(x) <= mu, \"LOW\").otherwise(\"HIGH\"))\n",
        "#             )\n",
        "\n",
        "# # 4) Analyze: group/pivot/correlations\n",
        "# # 4a) Null report\n",
        "# null_report = clean.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in clean.columns])\n",
        "# print(\"\\n--- Null report ---\"); null_report.show()\n",
        "\n",
        "# # 4b) If at least two numeric columns exist â†’ corr matrix (pairwise)\n",
        "# num_cols = [c for c, t in clean.dtypes if t in (\"int\",\"bigint\",\"double\",\"float\",\"long\",\"decimal\")]\n",
        "# if len(num_cols) >= 2:\n",
        "#     pairs = []\n",
        "#     for i in range(len(num_cols)):\n",
        "#         for j in range(i+1, len(num_cols)):\n",
        "#             c1, c2 = num_cols[i], num_cols[j]\n",
        "#             corr_val = clean.select(F.corr(c1, c2).alias(\"corr\")).first()[\"corr\"]\n",
        "#             pairs.append((c1, c2, corr_val))\n",
        "#     corr_df = spark.createDataFrame(pairs, schema=\"col1 string, col2 string, corr double\")\n",
        "#     print(\"\\n--- Pairwise correlations ---\"); corr_df.show(truncate=False)\n",
        "\n",
        "# # 4c) Quick frequency table (first string column vs first numeric column avg)\n",
        "# str_cols = [c for c, t in clean.dtypes if t == \"string\"]\n",
        "# if str_cols and num_cols:\n",
        "#     ccat, cnum = str_cols[0], num_cols[0]\n",
        "#     freq = (clean.groupBy(ccat).agg(\n",
        "#                 F.count(\"*\").alias(\"rows\"),\n",
        "#                 F.round(F.avg(cnum),2).alias(f\"avg_{cnum}\")\n",
        "#            ).orderBy(F.col(\"rows\").desc()))\n",
        "#     print(f\"\\n--- Frequency of {ccat} with avg({cnum}) ---\"); freq.show()\n",
        "\n",
        "# # 5) Export results\n",
        "# out_project = \"/content/project_output\"\n",
        "# (clean.limit(1000).write.mode(\"overwrite\").parquet(f\"{out_project}/clean_parquet\"))\n",
        "# freq.limit(1000).write.mode(\"overwrite\").option(\"header\", True).csv(f\"{out_project}/freq_csv\")\n",
        "# print(\"âœ… Project outputs saved under:\", out_project)\n",
        "# # \"\"\"\n",
        "\n",
        "\n",
        "# print(\"\\nðŸŽ¯ All sections ready. Comment/Uncomment blocks for your daily practice. Happy Learning!\")\n"
      ],
      "metadata": {
        "id": "OEPPTGZN9w6N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}