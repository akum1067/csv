{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MVu7zdZp_FZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b4053e2-0149-4b2f-b8a3-388f803781d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "# Install PySpark (Colab ke liye)\n",
        "# !pip install pyspark\n",
        "\n",
        "# Import Pandas and PySpark\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark session create karte hain\n",
        "spark = SparkSession.builder.appName(\"Beginner_Data_Analysis\").getOrCreate()\n",
        "print(\"Spark Ready âœ…\")\n"
      ],
      "metadata": {
        "id": "WvzGGXwg_Igq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tumhara GitHub repo ka base URL\n",
        "base = \"https://raw.githubusercontent.com/theabhinaykumar/csv/main/\"\n",
        "\n",
        "# Saare file links\n",
        "user_reviews   = base + \"user_reviews.csv\"\n",
        "apps           = base + \"apps.csv\"\n",
        "db1_sales      = base + \"DB1%20sales.csv\"\n",
        "hospital       = base + \"hospital.csv\"\n",
        "ab_nyc_2019    = base + \"AB_NYC_2019.csv\"\n",
        "creditcard     = base + \"creditcard.csv\"\n",
        "creditcard_1   = base + \"creditcard%201.csv\"\n",
        "\n",
        "print(user_reviews)\n"
      ],
      "metadata": {
        "id": "Yf0TPmMD_I30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas me CSV load karna\n",
        "df = pd.read_csv(user_reviews)\n",
        "\n",
        "# Top 5 rows dekhna\n",
        "print(df.head())\n",
        "\n",
        "# Shape of dataset\n",
        "print(\"Rows:\", df.shape[0])\n",
        "print(\"Columns:\", df.shape[1])\n",
        "\n",
        "# Column names\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "id": "9eM088LfI0wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark me CSV load karna\n",
        "sdf = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(user_reviews)\n",
        "\n",
        "# Top 5 rows dekhna\n",
        "sdf.show(5)\n",
        "\n",
        "# Schema dekhna\n",
        "sdf.printSchema()\n"
      ],
      "metadata": {
        "id": "4JhIa5F3I3L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset ka info\n",
        "print(df.info())\n",
        "\n",
        "# Summary statistics\n",
        "print(df.describe())\n",
        "\n",
        "# Null values count\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Specific columns select\n",
        "print(df[['reviewer_id', 'rating']].head())\n",
        "\n",
        "# Rows filter karna (rating > 4)\n",
        "print(df[df['rating'] > 4])\n"
      ],
      "metadata": {
        "id": "vps7fZGCI5p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Row count\n",
        "print(\"Rows:\", sdf.count())\n",
        "\n",
        "# Column names\n",
        "print(\"Columns:\", sdf.columns)\n",
        "\n",
        "# Summary statistics\n",
        "sdf.describe().show()\n",
        "\n",
        "# Null values count\n",
        "sdf.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in sdf.columns]).show()\n",
        "\n",
        "# Specific columns select\n",
        "sdf.select(\"reviewer_id\", \"rating\").show(5)\n",
        "\n",
        "# Rows filter karna (rating > 4)\n",
        "sdf.filter(F.col(\"rating\") > 4).show(5)\n"
      ],
      "metadata": {
        "id": "VwQhyNEKI8GG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ============================================================\n",
        "# # ALL-IN-ONE: Beginner Data Analysis (Pandas + PySpark)\n",
        "# # Works in Google Colab. Toggle ENGINE and FILE_NAME only.\n",
        "# # ============================================================\n",
        "\n",
        "# # --------------------\n",
        "# # 0) USER SETTINGS\n",
        "# # --------------------\n",
        "# ENGINE    = \"pandas\"     # \"pandas\" or \"pyspark\"\n",
        "# SOURCE    = \"github\"     # keep \"github\" for your repo\n",
        "# FILE_NAME = \"user_reviews.csv\"   # choose from FILES list below\n",
        "\n",
        "# # --------------------\n",
        "# # 1) FILE LIST / URLs\n",
        "# # --------------------\n",
        "# from urllib.parse import quote\n",
        "\n",
        "# GITHUB_USER   = \"theabhinaykumar\"\n",
        "# GITHUB_REPO   = \"csv\"\n",
        "# GITHUB_BRANCH = \"main\"\n",
        "\n",
        "# FILES = [\n",
        "#     \"New York 3,67,000+ Email.csv.xlsx\",\n",
        "#     \"apps.csv\",\n",
        "#     \"DB1 sales.csv\",\n",
        "#     \"hospital.csv\",\n",
        "#     \"AB_NYC_2019.csv\",\n",
        "#     \"user_reviews.csv\",\n",
        "#     \"creditcard.csv\",\n",
        "#     \"creditcard 1.csv\",\n",
        "# ]\n",
        "\n",
        "# def raw_url(user, repo, branch, filename):\n",
        "#     return f\"https://raw.githubusercontent.com/{user}/{repo}/{branch}/{quote(filename)}\"\n",
        "\n",
        "# URLS = {name: raw_url(GITHUB_USER, GITHUB_REPO, GITHUB_BRANCH, name) for name in FILES}\n",
        "\n",
        "# # --------------------\n",
        "# # 2) INSTALL / IMPORT\n",
        "# # --------------------\n",
        "# import sys, subprocess, os\n",
        "# def pip_install(pkgs):\n",
        "#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs])\n",
        "\n",
        "# # Always need pandas\n",
        "# pip_install([\"pandas\", \"openpyxl\"])\n",
        "# import pandas as pd\n",
        "\n",
        "# # PySpark only if chosen\n",
        "# if ENGINE.lower() == \"pyspark\":\n",
        "#     pip_install([\"pyspark==3.5.1\"])\n",
        "#     from pyspark.sql import SparkSession, functions as F\n",
        "\n",
        "# print(\"âœ… Engine:\", ENGINE)\n",
        "# print(\"ðŸ“„ Using file:\", FILE_NAME)\n",
        "# print(\"ðŸ”— RAW URL:\", URLS.get(FILE_NAME, \"(custom)\"))\n",
        "\n",
        "# # --------------------\n",
        "# # 3) LOAD DATA\n",
        "# # --------------------\n",
        "# if ENGINE == \"pandas\":\n",
        "#     # CSV vs Excel handled automatically by extension\n",
        "#     if FILE_NAME.lower().endswith((\".xlsx\", \".xls\")):\n",
        "#         df = pd.read_excel(URLS[FILE_NAME], engine=\"openpyxl\")\n",
        "#     else:\n",
        "#         df = pd.read_csv(URLS[FILE_NAME])\n",
        "#     print(\"\\n=== LOADED (pandas) ===\")\n",
        "#     print(\"Shape:\", df.shape)\n",
        "#     print(\"Columns:\", list(df.columns))\n",
        "#     print(df.head())\n",
        "\n",
        "# else:  # PySpark\n",
        "#     spark = (SparkSession.builder.appName(\"Beginner_All_In_One\").getOrCreate())\n",
        "#     print(\"\\nSpark version:\", spark.version)\n",
        "\n",
        "#     # For Excel in GitHub: simple path = pandas -> spark\n",
        "#     if FILE_NAME.lower().endswith((\".xlsx\", \".xls\")):\n",
        "#         pdf = pd.read_excel(URLS[FILE_NAME], engine=\"openpyxl\")\n",
        "#         sdf = spark.createDataFrame(pdf)\n",
        "#     else:\n",
        "#         sdf = (spark.read\n",
        "#                .option(\"header\", True)\n",
        "#                .option(\"inferSchema\", True)\n",
        "#                .csv(URLS[FILE_NAME]))\n",
        "#     print(\"\\n=== LOADED (pyspark) ===\")\n",
        "#     print(\"Columns:\", sdf.columns)\n",
        "#     sdf.show(5, truncate=False)\n",
        "\n",
        "# # --------------------\n",
        "# # 4) BASIC ANALYSIS\n",
        "# # --------------------\n",
        "# if ENGINE == \"pandas\":\n",
        "#     print(\"\\n=== BASIC ANALYSIS (pandas) ===\")\n",
        "#     # Info\n",
        "#     print(\"\\n-- info() --\")\n",
        "#     print(df.info())\n",
        "\n",
        "#     # Summary (numeric)\n",
        "#     print(\"\\n-- describe() --\")\n",
        "#     print(df.describe(include='all'))\n",
        "\n",
        "#     # Null counts\n",
        "#     print(\"\\n-- null counts --\")\n",
        "#     print(df.isna().sum())\n",
        "\n",
        "#     # Pick first numeric column (if any) for quick demos\n",
        "#     num_col = None\n",
        "#     for c in df.columns:\n",
        "#         if pd.api.types.is_numeric_dtype(df[c]):\n",
        "#             num_col = c\n",
        "#             break\n",
        "\n",
        "#     # Select first 2-3 columns safely\n",
        "#     print(\"\\n-- select first columns --\")\n",
        "#     print(df[df.columns[:3]].head())\n",
        "\n",
        "#     # Filter (if numeric column exists)\n",
        "#     if num_col:\n",
        "#         mean_val = df[num_col].mean()\n",
        "#         print(f\"\\n-- filter: {num_col} > mean ({mean_val:.3f}) --\")\n",
        "#         print(df[df[num_col] > mean_val].head())\n",
        "\n",
        "#         # Sort\n",
        "#         print(f\"\\n-- sort by {num_col} desc --\")\n",
        "#         print(df.sort_values(by=num_col, ascending=False).head())\n",
        "\n",
        "#     # Save small outputs (optional)\n",
        "#     df.head(100).to_csv(\"sample_output_pandas.csv\", index=False)\n",
        "#     print(\"\\nðŸ’¾ Saved: sample_output_pandas.csv\")\n",
        "\n",
        "# else:\n",
        "#     print(\"\\n=== BASIC ANALYSIS (pyspark) ===\")\n",
        "#     # Schema / count\n",
        "#     print(\"\\n-- schema --\")\n",
        "#     sdf.printSchema()\n",
        "#     print(\"\\n-- row count --\")\n",
        "#     print(sdf.count())\n",
        "\n",
        "#     # Summary (numeric)\n",
        "#     print(\"\\n-- describe() --\")\n",
        "#     sdf.describe().show()\n",
        "\n",
        "#     # Null counts\n",
        "#     print(\"\\n-- null counts --\")\n",
        "#     sdf.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in sdf.columns]).show()\n",
        "\n",
        "#     # Pick first numeric column\n",
        "#     num_col = None\n",
        "#     for c, t in sdf.dtypes:\n",
        "#         if t in (\"int\",\"bigint\",\"double\",\"float\",\"long\",\"decimal\"):\n",
        "#             num_col = c\n",
        "#             break\n",
        "\n",
        "#     # Select first columns\n",
        "#     print(\"\\n-- select first columns --\")\n",
        "#     sdf.select(*sdf.columns[:3]).show(5)\n",
        "\n",
        "#     # Filter & sort (if numeric column exists)\n",
        "#     if num_col:\n",
        "#         avg_val = sdf.select(F.avg(F.col(num_col)).alias(\"avg\")).first()[\"avg\"]\n",
        "#         print(f\"\\n-- filter: {num_col} > mean ({avg_val}) --\")\n",
        "#         sdf.filter(F.col(num_col) > avg_val).show(5)\n",
        "\n",
        "#         print(f\"\\n-- sort by {num_col} desc --\")\n",
        "#         sdf.orderBy(F.col(num_col).desc()).show(5)\n",
        "\n",
        "#     # Save small outputs (optional)\n",
        "#     (sdf.limit(100).write.mode(\"overwrite\").option(\"header\", True).csv(\"sample_output_spark\"))\n",
        "#     print(\"\\nðŸ’¾ Saved folder: sample_output_spark/\")\n",
        "\n",
        "# # --------------------\n",
        "# # 5) MINI PRACTICE (DO THESE)\n",
        "# # --------------------\n",
        "# print(\"\\nðŸŽ¯ PRACTICE (Beginner):\")\n",
        "# print(\"1) Change FILE_NAME to 'apps.csv' then run.\")\n",
        "# print(\"2) Change ENGINE to 'pyspark' then run.\")\n",
        "# print(\"3) Find the first numeric column and print its mean.\")\n",
        "# print(\"4) Filter rows where that numeric column > its mean.\")\n",
        "# print(\"5) Sort by that numeric column (descending) and show top 5.\")\n"
      ],
      "metadata": {
        "id": "bjt20N7bKeRg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}