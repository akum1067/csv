{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVu7zdZp_FZx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WvzGGXwg_Igq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ================================================================\n",
        "# # 🐼 PANDAS MEGA PRACTICE NOTEBOOK (0 → Beginner → Intermediate → Advanced)\n",
        "# # One-cell, comment/uncomment based learning plan (Colab-friendly)\n",
        "# # Author: Your Pandas Buddy\n",
        "# # ================================================================\n",
        "\n",
        "# # =========================\n",
        "# # 0) SETUP & DATA LOADING\n",
        "# # =========================\n",
        "# # Colab me ye libs usually preinstalled hoti hain; safety ke liye:\n",
        "# # !pip -q install pandas pyarrow fastparquet openpyxl lxml xlrd sqlalchemy duckdb sqlite-utils\n",
        "\n",
        "import os, io, json, math, random, textwrap, duckdb, sqlite3, numpy as np, pandas as pd\n",
        "from datetime import datetime, date, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 100)\n",
        "pd.set_option(\"display.width\", 120)\n",
        "print(\"✅ pandas:\", pd.__version__)\n",
        "\n",
        "\n",
        "# import pandas as pd\n",
        "\n",
        "# ==== GitHub RAW URLs ====\n",
        "base = \"https://raw.githubusercontent.com/theabhinaykumar/csv/main/\"\n",
        "\n",
        "user_reviews   = base + \"user_reviews.csv\"\n",
        "apps           = base + \"apps.csv\"\n",
        "db1_sales      = base + \"DB1%20sales.csv\"\n",
        "hospital       = base + \"hospital.csv\"\n",
        "ab_nyc_2019    = base + \"AB_NYC_2019.csv\"\n",
        "creditcard     = base + \"creditcard.csv\"\n",
        "creditcard_1   = base + \"creditcard%201.csv\"\n",
        "ny_367k_emails = base + \"New%20York%203,67,000%2B%20Email.csv.xlsx\"  # Excel file\n",
        "\n",
        "# ==== Load CSVs ====\n",
        "df_reviews   = pd.read_csv(user_reviews)\n",
        "df_apps      = pd.read_csv(apps)\n",
        "df_db1sales  = pd.read_csv(db1_sales)\n",
        "df_hospital  = pd.read_csv(hospital)\n",
        "df_abnyc     = pd.read_csv(ab_nyc_2019)\n",
        "df_credit    = pd.read_csv(creditcard)\n",
        "df_credit_1  = pd.read_csv(creditcard_1)\n",
        "\n",
        "# ==== Load Excel ====\n",
        "df_ny_emails = pd.read_excel(ny_367k_emails, engine=\"openpyxl\")\n",
        "\n",
        "# ==== Check data ====\n",
        "print(df_reviews.shape)\n",
        "print(df_apps.shape)\n",
        "print(df_hospital.shape)\n",
        "print(df_abnyc.shape)\n",
        "print(df_credit.shape)\n",
        "print(df_credit_1.shape)\n",
        "print(df_ny_emails.shape)\n",
        "\n",
        "# Peek data\n",
        "print(df_reviews.head())\n",
        "\n",
        "\n",
        "\n",
        "# # ---- Choose your data SOURCE ----\n",
        "# # \"github\" → public RAW CSV URL\n",
        "# # \"local_upload\" → manual upload from your computer\n",
        "# # \"drive\" → file path in mounted Google Drive\n",
        "# SOURCE = \"github\"   # ← change to: \"local_upload\" or \"drive\"\n",
        "\n",
        "# # Public small dataset (you can replace with your own RAW link)\n",
        "# GITHUB_RAW_CSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv\"  # sample\n",
        "\n",
        "# # Google Drive path (change if using \"drive\")\n",
        "# DRIVE_CSV_PATH = \"/content/drive/MyDrive/data/myfile.csv\"\n",
        "\n",
        "# def load_from_github(url: str) -> pd.DataFrame:\n",
        "#     \"\"\"\n",
        "#     GitHub CSV load via pandas over HTTPS\n",
        "#     \"\"\"\n",
        "#     df = pd.read_csv(url)\n",
        "#     return df\n",
        "\n",
        "# def load_from_local_upload() -> pd.DataFrame:\n",
        "#     \"\"\"\n",
        "#     Upload CSV from your laptop into Colab.\n",
        "#     \"\"\"\n",
        "#     from google.colab import files\n",
        "#     up = files.upload()\n",
        "#     if not up:\n",
        "#         raise RuntimeError(\"No file uploaded.\")\n",
        "#     fname = list(up.keys())[0]\n",
        "#     print(\"Uploaded:\", fname)\n",
        "#     df = pd.read_csv(fname)  # try default; adjust with sep=',' / encoding if needed\n",
        "#     return df\n",
        "\n",
        "# def load_from_drive(path: str) -> pd.DataFrame:\n",
        "#     \"\"\"\n",
        "#     Read CSV from mounted Drive path\n",
        "#     \"\"\"\n",
        "#     if not os.path.isdir(\"/content/drive\"):\n",
        "#         from google.colab import drive\n",
        "#         drive.mount(\"/content/drive\", force_remount=True)\n",
        "#     df = pd.read_csv(path)\n",
        "#     return df\n",
        "\n",
        "# # ---- Load main DataFrame: df ----\n",
        "# if SOURCE == \"github\":\n",
        "#     df = load_from_github(GITHUB_RAW_CSV_URL)\n",
        "# elif SOURCE == \"local_upload\":\n",
        "#     df = load_from_local_upload()\n",
        "# elif SOURCE == \"drive\":\n",
        "#     df = load_from_drive(DRIVE_CSV_PATH)\n",
        "# else:\n",
        "#     raise ValueError(\"SOURCE must be 'github', 'local_upload', or 'drive'\")\n",
        "\n",
        "# print(\"\\n=== DATA PREVIEW ===\")\n",
        "# print(\"Shape:\", df.shape)        # (rows, cols)\n",
        "# print(\"Columns:\", list(df.columns))\n",
        "# display(df.head())              # top 5\n",
        "# display(df.sample(min(5, len(df))))  # random sample (up to 5)\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # DAY 1 — BASICS: Inspect, Select, Filter, Sort, New Columns\n",
        "# # =========================================================\n",
        "# # UNCOMMENT to practice Day-1\n",
        "# # \"\"\"\n",
        "# print(\"\\n================= DAY 1 =================\")\n",
        "\n",
        "# # 1.1 Inspect\n",
        "# print(\"\\n-- Inspect --\")\n",
        "# print(df.info())\n",
        "# display(df.describe(include=\"all\"))\n",
        "# display(df.nunique())\n",
        "\n",
        "# # 1.2 Column selection\n",
        "# print(\"\\n-- Column selection --\")\n",
        "# some_cols = list(df.columns)[:3]\n",
        "# display(df[some_cols].head())\n",
        "\n",
        "# # 1.3 Row filtering (boolean indexing)\n",
        "# print(\"\\n-- Filtering (example) --\")\n",
        "# num_col = None\n",
        "# for c in df.columns:\n",
        "#     if pd.api.types.is_numeric_dtype(df[c]):\n",
        "#         num_col = c\n",
        "#         break\n",
        "# if num_col:\n",
        "#     avg_val = df[num_col].mean()\n",
        "#     display(df[df[num_col] > avg_val].head())\n",
        "\n",
        "# # 1.4 Sort values\n",
        "# print(\"\\n-- Sort values --\")\n",
        "# if num_col:\n",
        "#     display(df.sort_values(by=num_col, ascending=False).head())\n",
        "\n",
        "# # 1.5 Create/transform columns with assign\n",
        "# print(\"\\n-- assign() demo --\")\n",
        "# if num_col:\n",
        "#     df1 = (df\n",
        "#            .assign(is_big = lambda d: d[num_col] > d[num_col].mean(),\n",
        "#                    zscore = lambda d: (d[num_col] - d[num_col].mean()) / d[num_col].std()))\n",
        "#     display(df1.head())\n",
        "\n",
        "# # 1.6 Rename columns (safe snake_case)\n",
        "# print(\"\\n-- Rename (snake_case) --\")\n",
        "# def snake(s: str) -> str:\n",
        "#     return (s.strip().lower()\n",
        "#               .replace(\" \", \"_\").replace(\"-\", \"_\")\n",
        "#               .replace(\"(\", \"\").replace(\")\", \"\"))\n",
        "# df_ren = df.copy()\n",
        "# df_ren.columns = [snake(c) for c in df_ren.columns]\n",
        "# print(df_ren.columns.tolist())\n",
        "# # \"\"\"\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # DAY 2 — Missing Values, Dtypes, Casting, Duplicates, Memory\n",
        "# # =========================================================\n",
        "# # UNCOMMENT to practice Day-2\n",
        "# # \"\"\"\n",
        "# print(\"\\n================= DAY 2 =================\")\n",
        "\n",
        "# # 2.1 Missing values\n",
        "# print(\"\\n-- Missing Values --\")\n",
        "# display(df_ren.isna().sum())\n",
        "# df_fill = df_ren.copy()\n",
        "# for c in df_fill.columns:\n",
        "#     if pd.api.types.is_numeric_dtype(df_fill[c]):\n",
        "#         df_fill[c] = df_fill[c].fillna(df_fill[c].median())\n",
        "#     else:\n",
        "#         df_fill[c] = df_fill[c].fillna(\"Unknown\")\n",
        "# display(df_fill.head())\n",
        "\n",
        "# # 2.2 Dtypes & casting\n",
        "# print(\"\\n-- Dtypes & Casting --\")\n",
        "# print(df_ren.dtypes)\n",
        "# # Example: convert object → category for strings with low cardinality\n",
        "# df_cast = df_ren.copy()\n",
        "# for c in df_cast.columns:\n",
        "#     if df_cast[c].dtype == \"object\" and df_cast[c].nunique() < len(df_cast)*0.5:\n",
        "#         df_cast[c] = df_cast[c].astype(\"category\")\n",
        "# print(df_cast.dtypes)\n",
        "\n",
        "# # 2.3 Remove duplicates\n",
        "# print(\"\\n-- Duplicates --\")\n",
        "# before = len(df_cast)\n",
        "# df_cast = df_cast.drop_duplicates()\n",
        "# after = len(df_cast)\n",
        "# print(f\"Removed {before - after} duplicate rows\")\n",
        "\n",
        "# # 2.4 Memory usage tips\n",
        "# print(\"\\n-- Memory Usage (MB) --\")\n",
        "# mem_mb = df_cast.memory_usage(deep=True).sum() / 1_048_576\n",
        "# print(f\"Memory: {mem_mb:.3f} MB\")\n",
        "\n",
        "# # 2.5 to_numeric with errors\n",
        "# print(\"\\n-- to_numeric --\")\n",
        "# # demo: create a messy column\n",
        "# df_cast[\"messy_num\"] = [\"10\",\"20\",\"x\",\"30.5\", None]\n",
        "# df_cast[\"messy_num_num\"] = pd.to_numeric(df_cast[\"messy_num\"], errors=\"coerce\")\n",
        "# display(df_cast[[\"messy_num\",\"messy_num_num\"]].head())\n",
        "# # \"\"\"\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # DAY 3 — GroupBy, Aggregations, Pivot, Crosstab, Apply\n",
        "# # =========================================================\n",
        "# # UNCOMMENT to practice Day-3\n",
        "# # \"\"\"\n",
        "# print(\"\\n================= DAY 3 =================\")\n",
        "\n",
        "# # Pick cat & num col\n",
        "# cat_col, num_col2 = None, None\n",
        "# for c in df_ren.columns:\n",
        "#     if pd.api.types.is_categorical_dtype(df_ren[c]) or df_ren[c].dtype == \"object\":\n",
        "#         cat_col = c; break\n",
        "# for c in df_ren.columns:\n",
        "#     if pd.api.types.is_numeric_dtype(df_ren[c]):\n",
        "#         num_col2 = c; break\n",
        "\n",
        "# # 3.1 Basic groupby\n",
        "# print(\"\\n-- groupby().agg() --\")\n",
        "# if cat_col and num_col2:\n",
        "#     gb = df_ren.groupby(cat_col).agg(\n",
        "#         rows = (num_col2, \"size\"),\n",
        "#         mean_val = (num_col2, \"mean\"),\n",
        "#         median_val = (num_col2, \"median\"),\n",
        "#         max_val = (num_col2, \"max\"),\n",
        "#         min_val = (num_col2, \"min\"),\n",
        "#     ).reset_index()\n",
        "#     display(gb.sort_values(\"rows\", ascending=False).head())\n",
        "\n",
        "# # 3.2 pivot_table\n",
        "# print(\"\\n-- pivot_table --\")\n",
        "# # Need two cats + one value; make a second cat col if needed\n",
        "# other_cat = None\n",
        "# for c in df_ren.columns:\n",
        "#     if c != cat_col and (pd.api.types.is_categorical_dtype(df_ren[c]) or df_ren[c].dtype == \"object\"):\n",
        "#         other_cat = c; break\n",
        "\n",
        "# if cat_col and other_cat and num_col2:\n",
        "#     pvt = pd.pivot_table(df_ren, values=num_col2, index=cat_col, columns=other_cat,\n",
        "#                          aggfunc=\"mean\", fill_value=0)\n",
        "#     display(pvt)\n",
        "\n",
        "# # 3.3 Crosstab (frequency table)\n",
        "# print(\"\\n-- pd.crosstab --\")\n",
        "# if cat_col and other_cat:\n",
        "#     ct = pd.crosstab(df_ren[cat_col], df_ren[other_cat])\n",
        "#     display(ct)\n",
        "\n",
        "# # 3.4 Apply on groups\n",
        "# print(\"\\n-- groupby().apply --\")\n",
        "# if cat_col and num_col2:\n",
        "#     def summarize(g):\n",
        "#         return pd.Series({\n",
        "#             \"rows\": len(g),\n",
        "#             \"mean\": g[num_col2].mean(),\n",
        "#             \"std\": g[num_col2].std()\n",
        "#         })\n",
        "#     display(df_ren.groupby(cat_col).apply(summarize).reset_index())\n",
        "# # \"\"\"\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # DAY 4 — Merge/Join/Concat, Reshape (melt, stack/unstack), MultiIndex\n",
        "# # =========================================================\n",
        "# # UNCOMMENT to practice Day-4\n",
        "# # \"\"\"\n",
        "# print(\"\\n================= DAY 4 =================\")\n",
        "\n",
        "# # Synthetic small tables for joins\n",
        "# customers = pd.DataFrame({\n",
        "#     \"cust_id\": [1,2,3,4],\n",
        "#     \"name\": [\"Amit\",\"Riya\",\"Karan\",\"Neha\"],\n",
        "#     \"city\": [\"Delhi\",\"Noida\",\"Gurgaon\",\"Delhi\"]\n",
        "# })\n",
        "# orders = pd.DataFrame({\n",
        "#     \"order_id\": [101,102,103,104],\n",
        "#     \"cust_id\": [1,1,2,5],   # 5 not in customers\n",
        "#     \"amount\":  [2999,1599,899,1200]\n",
        "# })\n",
        "\n",
        "# print(\"-- customers --\"); display(customers)\n",
        "# print(\"-- orders --\"); display(orders)\n",
        "\n",
        "# # 4.1 Joins: inner/left/right/outer\n",
        "# inner = customers.merge(orders, on=\"cust_id\", how=\"inner\")\n",
        "# left  = customers.merge(orders, on=\"cust_id\", how=\"left\")\n",
        "# right = customers.merge(orders, on=\"cust_id\", how=\"right\")\n",
        "# outer = customers.merge(orders, on=\"cust_id\", how=\"outer\", indicator=True)\n",
        "\n",
        "# print(\"\\n-- INNER --\"); display(inner)\n",
        "# print(\"\\n-- LEFT --\"); display(left)\n",
        "# print(\"\\n-- RIGHT --\"); display(right)\n",
        "# print(\"\\n-- OUTER --\"); display(outer)\n",
        "\n",
        "# # 4.2 Concat (vertical & horizontal)\n",
        "# print(\"\\n-- Concat --\")\n",
        "# customers2 = pd.DataFrame({\"cust_id\":[5,6], \"name\":[\"Meera\",\"Vikram\"], \"city\":[\"Ghaziabad\",\"Faridabad\"]})\n",
        "# vcat = pd.concat([customers, customers2], ignore_index=True)\n",
        "# hcat = pd.concat([customers.set_index(\"cust_id\"), customers2.set_index(\"cust_id\")], axis=1)\n",
        "# display(vcat); display(hcat)\n",
        "\n",
        "# # 4.3 Melt (wide → long)\n",
        "# print(\"\\n-- Melt --\")\n",
        "# wide = pd.DataFrame({\n",
        "#     \"id\":[1,2,3],\n",
        "#     \"Apr\":[10,20,30],\n",
        "#     \"May\":[15,25,35],\n",
        "#     \"Jun\":[18,22,31]\n",
        "# })\n",
        "# display(wide)\n",
        "# long = wide.melt(id_vars=\"id\", var_name=\"month\", value_name=\"value\")\n",
        "# display(long)\n",
        "\n",
        "# # 4.4 Stack/Unstack with MultiIndex\n",
        "# print(\"\\n-- Stack/Unstack --\")\n",
        "# mi = long.set_index([\"id\",\"month\"]).sort_index()\n",
        "# unstacked = mi.unstack(\"month\")      # back to wide\n",
        "# restacked = unstacked.stack(\"month\") # back to long-style\n",
        "# display(unstacked)\n",
        "# display(restacked)\n",
        "# # \"\"\"\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # DAY 5 — Datetime, Time Series, Rolling/Expanding, String Ops, Category\n",
        "# # =========================================================\n",
        "# # UNCOMMENT to practice Day-5\n",
        "# # \"\"\"\n",
        "# print(\"\\n================= DAY 5 =================\")\n",
        "\n",
        "# # 5.1 Datetime parsing & features\n",
        "# print(\"\\n-- Datetime basics --\")\n",
        "# dates = pd.date_range(\"2025-08-20\", periods=10, freq=\"D\")\n",
        "# ts = pd.DataFrame({\"date\": dates, \"sales\": np.random.randint(50, 150, size=10)})\n",
        "# ts[\"week\"]  = ts[\"date\"].dt.isocalendar().week\n",
        "# ts[\"month\"] = ts[\"date\"].dt.month\n",
        "# ts[\"dow\"]   = ts[\"date\"].dt.day_name()\n",
        "# display(ts.head())\n",
        "\n",
        "# # 5.2 Resample (needs DatetimeIndex)\n",
        "# print(\"\\n-- Resample (W) --\")\n",
        "# ts2 = ts.set_index(\"date\").resample(\"W\").sum(numeric_only=True)\n",
        "# display(ts2)\n",
        "\n",
        "# # 5.3 Rolling window\n",
        "# print(\"\\n-- Rolling mean (3-day) --\")\n",
        "# ts[\"roll3\"] = ts[\"sales\"].rolling(3, min_periods=1).mean()\n",
        "# display(ts)\n",
        "\n",
        "# # 5.4 Expanding (cumulative)\n",
        "# print(\"\\n-- Expanding mean --\")\n",
        "# ts[\"exp_mean\"] = ts[\"sales\"].expanding().mean()\n",
        "# display(ts)\n",
        "\n",
        "# # 5.5 String ops (str methods)\n",
        "# print(\"\\n-- String ops --\")\n",
        "# s = pd.Series([\"  Delhi  \", \"noida\", None, \"Gurgaon\"])\n",
        "# display(pd.DataFrame({\n",
        "#     \"raw\": s,\n",
        "#     \"strip\": s.str.strip(),\n",
        "#     \"upper\": s.str.upper(),\n",
        "#     \"contains_no\": s.str.contains(\"no\", case=False, na=False)\n",
        "# }))\n",
        "\n",
        "# # 5.6 Categorical dtype\n",
        "# print(\"\\n-- Categorical --\")\n",
        "# cats = pd.Series([\"High\",\"Low\",\"Medium\",\"High\",\"Low\"], dtype=\"category\")\n",
        "# cats = cats.cat.set_categories([\"Low\",\"Medium\",\"High\"], ordered=True)\n",
        "# display(cats)\n",
        "# # \"\"\"\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # DAY 6 — Apply/Map/Applymap, Vectorization, Eval/Query, Plotting\n",
        "# # =========================================================\n",
        "# # UNCOMMENT to practice Day-6\n",
        "# # \"\"\"\n",
        "# print(\"\\n================= DAY 6 =================\")\n",
        "\n",
        "# # 6.1 apply on Series & DataFrame\n",
        "# print(\"\\n-- apply / map / applymap --\")\n",
        "# demo = pd.DataFrame({\"x\":[1,2,3,4,5], \"y\":[10,20,30,40,50]})\n",
        "# demo[\"x2\"] = demo[\"x\"].map(lambda z: z*z)\n",
        "# demo[\"xy\"] = demo.apply(lambda r: r[\"x\"]*r[\"y\"], axis=1)\n",
        "# display(demo)\n",
        "\n",
        "# # 6.2 Vectorization vs apply\n",
        "# print(\"\\n-- Vectorization --\")\n",
        "# demo[\"fast_xy\"] = demo[\"x\"]*demo[\"y\"]  # vectorized, faster\n",
        "# display(demo)\n",
        "\n",
        "# # 6.3 eval & query\n",
        "# print(\"\\n-- eval & query --\")\n",
        "# demo = demo.eval(\"z = x + y\")\n",
        "# display(demo.query(\"z > 30\"))\n",
        "\n",
        "# # 6.4 Plotting (Matplotlib only, no seaborn)\n",
        "# print(\"\\n-- Plotting --\")\n",
        "# plt.figure()\n",
        "# demo.plot(x=\"x\", y=\"y\", kind=\"line\", title=\"Simple Line\")\n",
        "# plt.show()\n",
        "\n",
        "# plt.figure()\n",
        "# demo.plot(x=\"x\", y=[\"y\",\"z\"], kind=\"bar\", title=\"Bar Demo\")\n",
        "# plt.show()\n",
        "# # \"\"\"\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # DAY 7 — I/O Everywhere: CSV, Excel, JSON, Parquet, Feather, SQL\n",
        "# # =========================================================\n",
        "# # UNCOMMENT to practice Day-7\n",
        "# # \"\"\"\n",
        "# print(\"\\n================= DAY 7 =================\")\n",
        "\n",
        "# OUT = \"/content/pandas_outputs\"\n",
        "# os.makedirs(OUT, exist_ok=True)\n",
        "\n",
        "# # 7.1 CSV\n",
        "# print(\"\\n-- CSV --\")\n",
        "# df_ren.to_csv(f\"{OUT}/data.csv\", index=False)\n",
        "# print(\"Wrote CSV:\", f\"{OUT}/data.csv\")\n",
        "# df_back = pd.read_csv(f\"{OUT}/data.csv\")\n",
        "# display(df_back.head())\n",
        "\n",
        "# # 7.2 Excel (openpyxl writer)\n",
        "# print(\"\\n-- Excel --\")\n",
        "# with pd.ExcelWriter(f\"{OUT}/data.xlsx\", engine=\"openpyxl\") as w:\n",
        "#     df_ren.to_excel(w, sheet_name=\"Sheet1\", index=False)\n",
        "# print(\"Wrote Excel:\", f\"{OUT}/data.xlsx\")\n",
        "\n",
        "# # 7.3 JSON (records)\n",
        "# print(\"\\n-- JSON --\")\n",
        "# df_ren.to_json(f\"{OUT}/data.json\", orient=\"records\", lines=False)\n",
        "# print(\"Wrote JSON:\", f\"{OUT}/data.json\")\n",
        "\n",
        "# # 7.4 Parquet (pyarrow/fastparquet)\n",
        "# print(\"\\n-- Parquet --\")\n",
        "# df_ren.to_parquet(f\"{OUT}/data.parquet\", engine=\"pyarrow\", index=False)\n",
        "# print(\"Wrote Parquet:\", f\"{OUT}/data.parquet\")\n",
        "# df_pq = pd.read_parquet(f\"{OUT}/data.parquet\")\n",
        "# display(df_pq.head())\n",
        "\n",
        "# # 7.5 Feather (fast columnar)\n",
        "# print(\"\\n-- Feather --\")\n",
        "# df_ren.to_feather(f\"{OUT}/data.feather\")\n",
        "# print(\"Wrote Feather:\", f\"{OUT}/data.feather\")\n",
        "\n",
        "# # 7.6 SQLite (SQL I/O)\n",
        "# print(\"\\n-- SQLite --\")\n",
        "# sql_path = f\"{OUT}/example.db\"\n",
        "# conn = sqlite3.connect(sql_path)\n",
        "# df_ren.to_sql(\"mytable\", conn, if_exists=\"replace\", index=False)\n",
        "# back = pd.read_sql_query(\"SELECT * FROM mytable LIMIT 5\", conn)\n",
        "# display(back)\n",
        "# conn.close()\n",
        "# print(\"Wrote SQLite DB:\", sql_path)\n",
        "# # \"\"\"\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # ADVANCED — Performance, Chunking, Read Options, Pipe, Styler, Testing\n",
        "# # =========================================================\n",
        "# # UNCOMMENT to practice ADVANCED\n",
        "# # \"\"\"\n",
        "# print(\"\\n================= ADVANCED =================\")\n",
        "\n",
        "# # A1) Efficient read_csv with dtypes, parse_dates, usecols\n",
        "# print(\"\\n-- Efficient read_csv --\")\n",
        "# # Suppose we know columns & dtypes; (here we reuse our CSV)\n",
        "# dtypes_map = {}\n",
        "# for c in df_ren.columns:\n",
        "#     if pd.api.types.is_integer_dtype(df_ren[c]):\n",
        "#         dtypes_map[c] = \"Int64\"   # nullable int\n",
        "#     elif pd.api.types.is_float_dtype(df_ren[c]):\n",
        "#         dtypes_map[c] = \"float32\"\n",
        "#     elif df_ren[c].dtype == \"object\" and df_ren[c].nunique() < len(df_ren)*0.5:\n",
        "#         dtypes_map[c] = \"category\"\n",
        "#     else:\n",
        "#         dtypes_map[c] = \"object\"\n",
        "\n",
        "# df_fast = pd.read_csv(f\"{OUT}/data.csv\",\n",
        "#                       dtype=dtypes_map,\n",
        "#                       parse_dates=[c for c in df_ren.columns if \"date\" in c.lower()],\n",
        "#                       usecols=df_ren.columns[:min(6, len(df_ren.columns))])\n",
        "# print(df_fast.dtypes)\n",
        "# display(df_fast.head())\n",
        "\n",
        "# # A2) Chunked processing (large files)\n",
        "# print(\"\\n-- Chunked read_csv --\")\n",
        "# tot_rows, sum_first_num = 0, 0.0\n",
        "# first_num = None\n",
        "# for c in df_ren.columns:\n",
        "#     if pd.api.types.is_numeric_dtype(df_ren[c]):\n",
        "#         first_num = c; break\n",
        "# if first_num:\n",
        "#     for chunk in pd.read_csv(f\"{OUT}/data.csv\", chunksize=2000):\n",
        "#         tot_rows += len(chunk)\n",
        "#         if first_num in chunk.columns:\n",
        "#             sum_first_num += chunk[first_num].fillna(0).sum()\n",
        "#     print(\"Total rows (chunked):\", tot_rows)\n",
        "#     print(f\"Sum({first_num}) over chunks:\", sum_first_num)\n",
        "\n",
        "# # A3) Pipe pattern for clean transforms\n",
        "# print(\"\\n-- pipe pattern --\")\n",
        "# def clean_cols(d: pd.DataFrame) -> pd.DataFrame:\n",
        "#     out = d.copy()\n",
        "#     out.columns = [snake(c) for c in out.columns]\n",
        "#     return out\n",
        "\n",
        "# def add_ratio(d: pd.DataFrame, a: str, b: str, outcol=\"ratio\") -> pd.DataFrame:\n",
        "#     if a in d.columns and b in d.columns:\n",
        "#         d[outcol] = d[a].astype(float) / d[b].replace(0, np.nan).astype(float)\n",
        "#     return d\n",
        "\n",
        "# piped = (df\n",
        "#          .pipe(clean_cols)\n",
        "#          .pipe(lambda d: d.assign(const=1))\n",
        "#          )\n",
        "# display(piped.head())\n",
        "\n",
        "# # A4) Style (for reporting)\n",
        "# print(\"\\n-- Styler --\")\n",
        "# styled = (piped\n",
        "#           .head(10)\n",
        "#           .style\n",
        "#           .highlight_max(axis=0)\n",
        "#           .format(precision=2))\n",
        "# display(styled)\n",
        "\n",
        "# # A5) Testing small invariants\n",
        "# print(\"\\n-- Simple assertions --\")\n",
        "# assert len(df) == len(df.drop_duplicates()), \"Data has duplicates! (Demo check; ignore if triggered)\"\n",
        "# print(\"Assertions demo (may raise above if not true) — OK to comment out.\")\n",
        "# # \"\"\"\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # MINI PROJECT — End-to-End (Load → Clean → Transform → Analyze → Export)\n",
        "# # =========================================================\n",
        "# # UNCOMMENT to practice MINI PROJECT\n",
        "# # \"\"\"\n",
        "# print(\"\\n================= MINI PROJECT =================\")\n",
        "\n",
        "# # 1) Load already done into df\n",
        "\n",
        "# # 2) Clean: standardize columns, trim, handle missing\n",
        "# def clean_basic(d: pd.DataFrame) -> pd.DataFrame:\n",
        "#     out = d.copy()\n",
        "#     out.columns = [snake(c) for c in out.columns]\n",
        "#     for c in out.columns:\n",
        "#         if out[c].dtype == \"object\":\n",
        "#             out[c] = out[c].astype(str).str.strip()\n",
        "#     # simple impute\n",
        "#     for c in out.columns:\n",
        "#         if pd.api.types.is_numeric_dtype(out[c]):\n",
        "#             out[c] = out[c].fillna(out[c].median())\n",
        "#         else:\n",
        "#             out[c] = out[c].replace([\"nan\",\"None\"], np.nan).fillna(\"Unknown\")\n",
        "#     return out\n",
        "\n",
        "# clean = clean_basic(df)\n",
        "# display(clean.head())\n",
        "\n",
        "# # 3) Transform: feature engineering demo\n",
        "# num_cols = [c for c in clean.columns if pd.api.types.is_numeric_dtype(clean[c])]\n",
        "# str_cols = [c for c in clean.columns if clean[c].dtype == \"object\"]\n",
        "# if num_cols:\n",
        "#     x = num_cols[0]\n",
        "#     clean[\"zscore\"] = (clean[x] - clean[x].mean()) / clean[x].std(ddof=0)\n",
        "#     clean[\"bucket\"] = np.where(clean[x] <= clean[x].mean(), \"LOW\", \"HIGH\")\n",
        "\n",
        "# # 4) Analyze\n",
        "# print(\"\\n-- Null report --\")\n",
        "# display(clean.isna().sum())\n",
        "\n",
        "# if str_cols and num_cols:\n",
        "#     ccat, cnum = str_cols[0], num_cols[0]\n",
        "#     grp = (clean.groupby(ccat, dropna=False)\n",
        "#                  .agg(rows=(cnum, \"size\"),\n",
        "#                       avg=(cnum, \"mean\"))\n",
        "#                  .sort_values(\"rows\", ascending=False)\n",
        "#                  .reset_index())\n",
        "#     display(grp.head())\n",
        "\n",
        "# # 5) Export\n",
        "# PROJECT_OUT = \"/content/pandas_project_output\"\n",
        "# os.makedirs(PROJECT_OUT, exist_ok=True)\n",
        "# clean.to_parquet(f\"{PROJECT_OUT}/clean.parquet\", index=False)\n",
        "# grp.to_csv(f\"{PROJECT_OUT}/group_summary.csv\", index=False)\n",
        "# print(\"✅ Project outputs at:\", PROJECT_OUT)\n",
        "# # \"\"\"\n",
        "\n",
        "\n",
        "# print(\"\\n🎯 All sections ready. Comment/Uncomment as you learn. Happy Pandas-ing! 🐼\")\n"
      ],
      "metadata": {
        "id": "Yf0TPmMD_I30"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}